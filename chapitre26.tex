\chapter{Espaces vectoriels euclidiens}
\label{chap:espaceseuclidiens}
\minitoc
\minilof
\minilot

\section{Produits scalaire}

\subsection{Notion de produit scalaire}

Soit un espace vectoriel \(E\).
\begin{defdef}
  On appelle produit scalaire sur \(E\) toute application
  \begin{equation}
    \fonction{\varphi}{E \times E}{\R}{(x,y)}{\prodscal{x}{y}},
  \end{equation}
  telle que~:
  \begin{enumerate}
    \item pour tout vecteurs \(x\) et \(y\) de \(E\), on a \(\prodscal{x}{y}=\prodscal{y}{x}\);
    \item pour tout vecteurs \(x\), \(x'\) et \(y\) de \(E\), et tout réels \(\lambda\), on a \(\prodscal{\lambda x +x'}{y} = \lambda\prodscal{x}{y} + \prodscal{x'}{y}\);
    \item pour tout vecteur \(x\) non nul de \(E\), \(\prodscal{x}{x}>0\).
  \end{enumerate}
\end{defdef}
\emph{Remarque}~:
\begin{itemize}
  \item La linéarité à droite est aussi vraie grâce à la symétrie, et alors le produit scalaire est une forme bilinéaire symétrique;
  \item on peut aussi remarquer que \(\prodscal{x}{x}=0\) si et seulement si \(x=0\); alors dans ce cas le produit scalaire est défini.
    \begin{proof}
      \(\implies\) Soit \(x \in E\), si \(x\neq 0\) alors \(\prodscal{x}{x}>0\) et \(\prodscal{x}{x}\geqslant 0\). Sinon, alors \(x=0\) et \(\prodscal{x}{x}=0\).

      \(\impliedby\) Si \(x \neq 0\) alors avec \(\prodscal{x}{x}\geqslant 0\), on a \(\prodscal{x}{x} \neq 0\). Donc \(\prodscal{x}{x}>0\).
    \end{proof}
\end{itemize}

Finalement, un produit scalaire est une forme bilinéaire symétrique et définie-positive.

\begin{prop}
  Soit  \((E,\varphi)\) un espace vectoriel réel muni d'un produit scalaire. Pour tout vecteur \(x \in E\), on a
  \begin{equation}
    \left(\forall y \in E \quad \prodscal{x}{y}=0\right) \iff x=0.
  \end{equation}
\end{prop}
\begin{proof}
  \(\impliedby\) Déjà vu.

  \(\implies\) On applique l'égalité en \(y=x\) et on a \(\prodscal{x}{x}=0\), qui est vrai si et seulement si \(x=0\). 
\end{proof}

\subsection{Norme euclidienne associée à un produit scalaire}

Soit  \((E,\varphi)\) un espace vectoriel réel muni d'un produit scalaire.

\subsubsection{Définition}

\begin{defdef}
  Pour tout vecteur \(x \in E\), on définit sa norme euclidienne par \(\norme{x}=\sqrt{\prodscal{x}{x}}\). Qui est légitime puisque \(\prodscal{x}{x} \geqslant 0\).
\end{defdef}

\danger La norme dépend du produit scalaire choisi.

\emph{Relation entre le produit scalaire et la norme}
Soient \(x\) et \(y\) deux vecteurs de \(E\) et \(\lambda\) et \(\mu\) deux réels, alors
\begin{gather}
  \norme{\lambda x + \mu y} = \lambda^2 \norme{x}^2 + \mu^2 \norme{y}^2 + 2\lambda\mu\prodscal{x}{y}; \\
  \norme{x + y} = \norme{x}^2 + \norme{y}^2 + 2\prodscal{x}{y}; \\
  \norme{x - y} = \norme{x}^2 + \norme{y}^2 - 2\prodscal{x}{y}; \\
  \prodscal{x}{y} = \frac{1}{4}\left(\norme{x+y}^2-\norme{x-y}^2 \right) = \frac{1}{2}\left(\norme{x+y}^2-\norme{x}^2-\norme{x}^2 \right);\\
  \norme{x + y}^2+\norme{x - y}^2 = 2\left(\norme{x}^2 + \norme{y}^2\right).
\end{gather}
\begin{proof}
  On démontre la première égalité grâce à la bilinéarité du produit scalaire et grâce à la définition de la norme. Les autres points se démontre facilement.
\end{proof}

\subsubsection{Inégalité de Cauchy-Schwarz}

\begin{theo}[Théorème de Cauchy-Schwarz]
  pour tout vecteurs \(x\) et \(y\) de \(E\), on a
  \begin{equation}
    \abs{\prodscal{x}{y}} \leqslant \norme{x}\norme{y}.
  \end{equation}
\end{theo}
\begin{proof}
  Soient \(x\) et \(y\) deux vecteurs de \(E\), on définit l'application
  \begin{equation}
    \fonction{P}{\R}{\R}{\lambda}{\norme{x+\lambda y}}.
  \end{equation}
  Elle vérifie~:
  \begin{enumerate}
    \item pour tout réel \(\lambda\), \(P(\lambda)\geqslant 0\);
    \item pour tout réel \(\lambda\), \(P(\lambda) = \norme{x}^2+\lambda^2\norme{y}^2+2\lambda\prodscal{x}{y}\).
  \end{enumerate}
  \(P\) est un polynôme de degré inférieur ou égal à \(2\). 

  Dans le cas où \(\norme{y}=0\), alors \(y=0\) et l'inégalité \(\abs{\prodscal{x}{y}} \leqslant \norme{x}\norme{y}\) est vraie.

  Dans le cas où \(\norme{y}\neq 0\), alors \(P\) est de degré \(2\) et soit \(\delta\) le discriminant réduit de \(P\). Comme \(P\) ne change pas de signe ce discriminant est négatif et \(\delta=\prodscal{x}{y}^2-\norme{x}^2\norme{y}^2\leqslant 0\). D'où l'inégalité \(\abs{\prodscal{x}{y}} \leqslant \norme{x}\norme{y}\).
\end{proof}
%
\begin{prop}
  pour tout vecteurs \(x\) et \(y\) de \(E\),
  \begin{equation}
    \abs{\prodscal{x}{y}}=\norme{x}\norme{y} \iff (x,y) \text{~est liée}.
  \end{equation}
\end{prop}
\begin{proof}
  \begin{align}
    \abs{\prodscal{x}{y}}=\norme{x}\norme{y} &\iff \begin{cases} \abs{\prodscal{x}{y}}=\norme{x}\norme{y} \text{~et~} x=0 \\ \text{~ou~} \\
    \abs{\prodscal{x}{y}}=\norme{x}\norme{y} \text{~et~} x\neq 0 \end{cases}\\
                                             &\iff \begin{cases}  x=0 \\ \text{~ou~} \\
                                             x\neq 0 \text{~et~}  \delta=0\end{cases}\\
                                             &\iff \begin{cases}  x=0 \\ \text{~ou~} \\
                                             x\neq 0 \text{~et~}  \exists \lambda_0 \in \R \quad P(\lambda_0)=0\end{cases}\\
                                             &\iff \begin{cases}  x=0 \\ \text{~ou~} \\
                                             x\neq 0 \text{~et~}  \exists \lambda_0 \in \R \quad x=-\lambda_0 y\end{cases}\\
                                             &\iff (x,y) \text{~est liée}.
    \end{align}
  \end{proof}

  \subsubsection{Inégalité de Minkowski}

  \begin{theo}
    Pour tout couple de  vecteurs \((x,y) \in E\) on a
    \begin{equation}
      \norme{x+y} \leqslant \norme{x}+\norme{y}.
    \end{equation}
  \end{theo}
  \begin{proof}
    Soit un couple de  vecteurs \((x,y) \in E\), alors en appliquant le théorème de Cauchy-Schwarz on a 
    \begin{align}
      \norme{x+y}^2 &= \norme{x}^2 + \norme{y}^2 + 2\prodscal{x}{y} \\
                    & \leqslant \norme{x}^2 + \norme{y}^2 + 2\abs{\prodscal{x}{y}} \\
                    & \leqslant \norme{x}^2 + \norme{y}^2 + 2\norme{x}\norme{y}=(\norme{x}+\norme{y})^2.
    \end{align}
    Comme \(\norme{x+y} \geqslant 0\), \(\norme{x}\geqslant 0\) et \(\norme{y}\ge 0\), le sens de l'inégalité est conservé en passant à la racine carrée et on a bien
    \begin{equation}
      \norme{x+y} \leqslant \norme{x}+\norme{y}.
    \end{equation}
  \end{proof}
  %
  \begin{prop}
    Pour tout couple de  vecteurs \((x,y) \in E\) on a
    \begin{equation}
      \norme{x+y} = \norme{x}+\norme{y} \iff x=0 \text{~ou~} (x\neq 0 \text{~et~} \exists \mu \geqslant 0  \quad y=\mu x)
    \end{equation}
  \end{prop}
  \begin{proof}
    Soit un couple de  vecteurs \((x,y) \in E\), alors
    \begin{align}
      \norme{x+y} = \norme{x}+\norme{y} &\iff \begin{cases} \prodscal{x}{y} = \abs{\prodscal{x}{y}} \\ \text{~et~} \\ \abs{\prodscal{x}{y}} = \norme{x}\norme{y}\end{cases} \\
                                        &\iff  \begin{cases}  \prodscal{x}{y} \geqslant 0 \\ \text{~et~} \\ \begin{cases} x=0 \\ \text{~ou~} \\ x\neq 0 \text{~et~} \exists \lambda \in \R \quad y=\lambda x\end{cases} \end{cases}\\
                                        & \iff \begin{cases} \prodscal{x}{y} \geqslant 0 \text{~et~} x =0 \\ \text{~ou~} \\  \prodscal{x}{y} \geqslant 0 \text{~et~} x \neq 0 \text{~et~} \exists \lambda \in \R \quad y=\lambda x \end{cases}\\
                                        & \iff \begin{cases} x=0 \\ \text{~ou~} \\ x \neq 0 \text{~et~} \exists \lambda \in \R \quad y=\lambda x \text{~et~} \lambda\norme{x}^2 \geqslant 0 \end{cases} \\
                                        & \iff \begin{cases} x=0 \\ \text{~ou~} \\ x \neq 0 \text{~et~} \exists \lambda \in \Rpluss \quad y=\lambda x \end{cases}
    \end{align}
  \end{proof}

  \emph{Conséquences~:} Soit un couple de  vecteurs \((x,y) \in E\), alors
  \begin{gather}
    \abs{\norme{x}-\norme{y}} \leqslant \norme{x+y}\\
    \abs{\norme{x}-\norme{y}} \leqslant \norme{x-y}.
  \end{gather}
  \begin{proof}
    On a
    \begin{equation}
      \norme{x} = \norme{x+y-y} \leqslant \norme{x+y} +\norme{y}
    \end{equation}
    alors
    \begin{equation}
      \norme{x} - \norme{y} \leqslant \norme{x+y}
    \end{equation}
    et en passant à la valeur absolue on a bien
    \begin{equation}
      \abs{\norme{x}-\norme{y}} \leqslant \norme{x+y}.
    \end{equation}
    On applique cette inégalité à \(-y\) pour avoir la deuxième.
  \end{proof}

  \subsubsection{Application norme euclidienne}

  Soit un \(\R\)-espace vectoriel \(E\) muni d'un produit scalaire \(\prodscal{.}{.}\), et soit \(\norme{.}\) la norme euclidienne associée.

  \begin{prop}
    L'application \(\fonction{\norme{.}}{E}{\R}{x}{\norme{x}}\) est une norme, c'est-à-dire qu'elle vérifie
    \begin{enumerate}
      \item la positivité : pour tout vecteur \(x \in E\), \(\norme{x} \geqslant 0\);
      \item la séparation : pour tout vecteur \(x \in E\), \(\norme{x}= 0 \implies x=0\);
      \item l'homogénéité : pour tout réel \(\lambda\) et tout vecteur \(x \in E\), \(\norme{\lambda x}=\abs{\lambda}\norme{x}\);
      \item l'inégalité triangulaire (ou de Minkowski) : pour tout couple de vecteurs \((x,y) \in E^2\), \(\norme{x+y}\leqslant \norme{x}+\norme{y}\).
    \end{enumerate}
  \end{prop}
  \begin{proof}
    La démonstration repose sur les propriété du produit scalaire, parce que pour tout vecteur \(x \in E\), \(\norme{x}^2 = \prodscal{x}{x}\).
  \end{proof}

  \subsubsection{Distance associée à la norme euclidienne}

  Avec les mêmes notations, on définit l'application distance
  \begin{equation}
    \fonction{d}{E^2}{\R}{(x,y)}{\norme{x-y}}.
  \end{equation}
  % 
  \begin{prop}
    L'application distance \(d\) vérifie les propriétés suivantes~: Pour tout triplet \((x,y,z) \in E^3\) on a
    \begin{enumerate}
      \item \(d(x,y) \in \Rpluss\);
      \item \(d(y,x)=d(x,y)\);
      \item \(d(x,y)=0 \implies x=y\);
      \item \(d(x,z)\leqslant d(x,y)+\d(y,z)\).
    \end{enumerate}
  \end{prop}

  \subsection{Exemples de produits scalaires}

  \subsubsection{Produit scalaire sur \(\R^n\)}

  Soit \(E=\R^n\), \(x=(x_i)_{1\le i\leqslant n}\) et \(y=(y_i)_{1\le i\leqslant n}\). On pose \(\prodscal{x}{y}=\sum_{i=1}^n x_iy_i\). Alors \(\fonction{}{\R^n\times\R^n}{\R}{(x,y)}{\prodscal{x}{y}}\) est un produit scalaire. C'est le produit scalaire canonique. La norme euclidienne associée est telle que
  \begin{equation}
    \forall x \in \R^n \quad \norme{x} = \sqrt{\sum_{i=1}^n x_i^2}.
  \end{equation}
  La distance associée est telle que
  \begin{equation}
    \forall (x,y) \in \R^n\times\R^n \quad d(x,y)=\norme{x-y} = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}.
  \end{equation}
  L'inégalité de Cauchy-Scharwz s'écrit
  \begin{equation}
    \forall (x,y) \in \R^n\times\R^n \quad \abs{\sum_{i=1}^n x_iy_i} \leqslant \sqrt{\sum_{i=1}^n x_i^2} \sqrt{\sum_{i=1}^n y_i^2}.
  \end{equation}
  L'inégalité de Minkowski s'écrit
  \begin{equation}
    \forall (x,y) \in \R^n\times\R^n \quad \sqrt{\sum_{i=1}^n (x_i+y_i)^2} \leqslant \sqrt{\sum_{i=1}^n x_i^2}+ \sqrt{\sum_{i=1}^n y_i^2}.
  \end{equation}

  \subsubsection{Produit scalaire sur \(\cont{\intervalleff{a}{b}}{\R}\)}

  Soient deux réels \(a\) et \(b\) tels que \(a<b\) et l'application
  \begin{equation}
    \fonction{\varphi}{\cont{\intervalleff{a}{b}}{\R}^2}{\R}{(f,g)}{\int_{[a,b]}fg}.
  \end{equation}
  L'application \(\varphi\) est bilinéaire, symétrique, et définie-positive. C'est-à-dire que c'est un produit scalaire. En effet pour toute fonction \(f\) continue sur \([a,b]\) on a bien
  \begin{enumerate}
    \item \(\int_{[a,b]}f^2 \geqslant 0\);
    \item \(\int_{[a,b]}f^2 = 0 \iff f^2=\tilde{0} \iff f=\tilde{0}\).
  \end{enumerate}


  \emph{Remarque~: Sur l'ensemble des fonctions continues par morceaux, \(\varphi\) n'est pas un produit scalaire car \(\varphi\) n'est pas définie positive. On dispose qand même de l'inégalité de Cauchy-Schwarz.}

  La norme euclidienne associée à \(\varphi\) est telle que
  \begin{equation}
    \forall f \in \cont{\intervalleff{a}{b}}{\R} \quad \norme{f}=\sqrt{\int_{[a,b]}f^2}.
  \end{equation}
  La distance euclidienne associée est 
  \begin{equation}
    \forall (f,g) \in \cont{\intervalleff{a}{b}}{\R}^2 \quad d(f,g)=\sqrt{\int_{[a,b]}(f-g)^2}.
  \end{equation}
  L'inégalité de Cauchy-Schwarz s'écrit
  \begin{equation}
    \forall (f,g) \in \cont{\intervalleff{a}{b}}{\R}^2 \quad \abs{\int_{[a,b]}fg} \leqslant \sqrt{\int_{[a,b]}f^2} \sqrt{\int_{[a,b]}g^2}.
  \end{equation}
  L'inégalité de Minkowski s'écrit
  \begin{equation}
    \forall (f,g) \in \cont{\intervalleff{a}{b}}{\R}^2 \quad \sqrt{\int_{[a,b]}(f+g)^2} \leqslant \sqrt{\int_{[a,b]}f^2} + \sqrt{\int_{[a,b]}g^2}.
  \end{equation}

  \emph{Remarque~:} Soit \(E\) l'ensemble des fonctions continues \(2\pi\)-périodiques de \(\R\) dans \(\R\). On définit l'application \(\fonction{\varphi}{E^2}{\R}{(f,g)}{\frac{1}{2\pi}\int_{\intervalleff{0}{2\pi}}fg}\). C'est un produit scalaire.

  \subsection{Familles de vecteurs orthogonales et familles de vecteurs orthonormales}

  Soit \(E\) un \(\R\)-espace vectoriel muni d'un produit scalaire \(\varphi\).

  \subsubsection{Vecteur unitaire}

  \begin{defdef}
    Soit \(x \in E\). On dit que \(x\) est unitaire si et seulement \(\norme{x}=1\). \emph{C'est une caractéristique qui dépend du produit scalaire \(\varphi\)}.
  \end{defdef}

  \emph{Remarque}~: Si \(x\) est non nul, alors \(\frac{x}{\norme{x}}\) est un vecteur unitaire colinaire à \(x\) et de même sens.

  \subsubsection{Vecteurs orthogonaux}

  \begin{defdef}
    Soient deux vecteurs \(x\) et \(y\) de \(E\). On dit que \(x\) et \(y\) sont orthogonaux, on note \(x \perp y\) si et seulement si \(\prodscal{x}{y}=0\).
  \end{defdef}

  \danger C'est une notion qui dépend du produit scalaire choisi.

  \begin{prop}
    \begin{gather}
      \forall (x,y) \in E^2 \quad x \perp y \iff y \perp x;\\
      \forall x \in E \quad x \perp 0;\\
      \forall x \in E \quad x \perp x \iff x=0;\\
      \forall x \in E \quad [(\forall y \in E \quad x \perp y) \iff x=0].
    \end{gather}
  \end{prop}

  Le seul vecteur orthogonal à tous les vecteurs de l'espace est le vecteur nul.
  \begin{proof}
    Il suffit de réécrire les propriétés déjà vues.
  \end{proof}
  \begin{theo}[Théorème de Pythagore]
    Pour tout couple de vecteurs \((x,y) \in E^2\), on a
    \begin{equation}
      x \perp y \iff \norme{x+y}^2 = \norme{x}^2 + \norme{y}^2.
    \end{equation}
  \end{theo}
  \begin{proof}
    \begin{align}
      x \perp y &\iff \prodscal{x}{y}=0 \\
                &\iff \frac{1}{2} (\norme{x+y}^2-\norme{x}^2-\norme{y}^2)=0 \\
                &\iff \norme{x+y}^2 = \norme{x}^2 + \norme{y}^2.
    \end{align}
  \end{proof}

  \subsubsection{Familles orthogonales, familles orthonormales}

  \begin{defdef}
    Soit un ensemble fini \(I\) ayant au moins deux éléments. Soit \(\X=(x_i)_{i \in I}\) une famille de vecteurs de \(E\) indéxée par \(I\).
    \begin{itemize}
      \item On dit que \(\X\) est une famille orthogonale de \(E\) (FOG) si et seulement si
        \begin{equation}
          \forall (i,j) \in I^2 \quad i\neq j \implies x_i \perp x_j.
        \end{equation}
        Les vecteurs sont deux à deux orthogonaux.
      \item On dit qe \(\X\) est une famille orthonormale de \(E\) (FON) si et seulement si \(\X\) est une famille orthogonale de \(E\) et si pour tout \(i \in I\), \(\norme{x_i}=1\).
    \end{itemize}
  \end{defdef}

  \begin{prop}
    Une famille orthogonale finie de vecteurs de \(E\) dont aucun n'est nul est libre.
  \end{prop}
  \begin{proof}
    Soit \(\X=(x_i)_{i \in I}\) une famille orthogonale finie de \(E\) et \((\alpha_i)_{i \in I} \in \R^I\) telle que \(\sum_{i \in I}\alpha_i x_i=0\). Soit \(i_0 \in I\), alors \(\prodscal{x_{i_0}}{0}=0\). Alors
    \begin{align}
      0 &=\prodscal{\sum_{i \in I}\alpha_i x_i}{x_{i_0}} \\
        &=\sum_{i \in I}\alpha_i \prodscal{x_i}{x_{i_0}} && \text{linéarité à gauche}\\
        &=\alpha_{i_0} \norme{x_{i_0}}^2. && \text{famille orthogonale}
    \end{align}
    Comme tous les vecteurs sont non nuls on a \(x_{i_0}\neq 0\) et donc \(\norme{x_{i_0}} \neq 0\). Si bien que \(\alpha_{i_0}=0\), qui est vrai pour tout \(i_0\), alors la famille \(\X\) est libre.
  \end{proof}

  \begin{theo}[Théorème de Pythagore pour une famille orthogonale finie]
    Soit un naturel \(n\geqslant 2\) et une famille orthogonale \((x_i)_{i \in \intervalleentier{1}{n}} \in E^n\). Alors
    \begin{equation}
      \norme{\sum_{i=1}^nx_i}^2 = \sum_{i=1}^n \norme{x_i}^2.
    \end{equation}
  \end{theo}
  \begin{proof}[Démonstration par récurrence]
    Initialement on prend \(n=2\) et c'est le théorème de Pythagore classique énoncé ci-avant. Soit un naturel \(n \ge 2\) et supposons l'assertion vraie au rang \(n\), montrons qu'elle est encore vraie au rang \(n+1\). Soit famille orthogonale \((x_i)_{i \in \intervalleentier{1}{n+1}} \in E^{n+1}\), alors
    \begin{equation}
      \norme{\sum_{i=1}^{n+1}x_i}^2 =  \norme{\sum_{i=1}^{n}x_i+x_{n+1}}^2.
    \end{equation}
    Or
    \begin{align}
      \prodscal{\sum_{i=1}^{n}x_i}{x_{n+1}} &=\sum_{i=1}^{n}\prodscal{x_i}{x_{n+1}} && \text{linéarité} \\
                                            &=0 && \text{famille orthogonale}.
    \end{align}
    Alors \(x_{n+1} \perp \sum_{i=1}^n x_i\). Donc
    \begin{align}
      \norme{\sum_{i=1}^{n+1}x_i}^2 &=  \norme{\sum_{i=1}^{n}x_i+x_{n+1}}^2 \\
                                    &=\norme{\sum_{i=1}^{n}x_i}^2 + \norme{x_{n+1}}^2 && x_{n+1} \perp \sum_{i=1}^n x_i \\
                                    &=\sum_{i=1}^{n+1}\norme{x_i}^2 &&  \text{Hypothèse de récurrence}.
    \end{align}
    L'assertion au rang \(n+1\) est vraie. Par théorème de récurrence, le résultat est vraie pour tout naturel \(n \geqslant 2\).
  \end{proof}

  \subsection{Orthogonalité de sous-espaces vectoriels}

  \subsubsection{Orthogonal}

  Soit \((E,\varphi)\) un \(\R\)-espace vectoriel muni d'un produit scalaire \(\varphi\).

  \begin{defdef}
    Soit \(F\) un sous-espace vectoriel de \(E\). Posons
    \begin{equation}
      F^{\perp}=\enstq{x \in E}{\forall y \in F \quad x \perp y}.
    \end{equation}
    C'est l'ensemble des vecteurs de \(E\) qui sont orthogonaux à tous les vecteurs de \(F\). \(F^{\perp}\) est appelé l'orthogonal de \(F\).
  \end{defdef}

  \emph{Exemples~:}
  \begin{itemize}
    \item Si \(F=E\) alors \(F^{\perp}=\{0\}\);
    \item Si \(F=\{0\}\) alors \(F^{\perp}=E\).
  \end{itemize}

  \begin{prop}
    Pour tout sous-espace vectoriel \(F\) de \(E\), \(F^{\perp}\) est un sous-espace vectoriel de \(E\).
  \end{prop}
  \begin{proof}
    Par définition \(F^\perp \subset E\). Comme \(0 \in F^\perp\), il est non vide. Soient deux vecteurs \(x\) et \(x'\) de \(F^\perp\) et \(\lambda \in \R\). Pour tout \(y \in F\) on a
    \begin{equation}
      \prodscal{\lambda x+x'}{y}=\lambda \prodscal{x}{y}+\prodscal{x'}{y}=0.
    \end{equation}
    alors \(\lambda x+x' \perp y\) et donc \(\lambda x+x' \in F^\perp\).

    Par caractérisation \(F^\perp\) est un sous-espace vectoriel de \(E\).
  \end{proof}

  Comme \(F^\perp\) est un sous-espace vectoriel, on peut définir son orthogonal \((F^\perp)^\perp\).

  \begin{prop}
    Pour tout sous-espace vectoriel \(F\) de \(E\), on a
    \begin{equation}
      F \subset (F^\perp)^\perp.
    \end{equation}
    \danger En général ce n'est pas une égalité.
  \end{prop}
  \begin{proof}
    On note \(G=F^\perp\). Soit \(x \in F\), alors
    \begin{equation}
      \forall y \in G \quad x\perp y.
    \end{equation}
    Alors \(x \in G^\perp\). Du coup \(F \subset G^\perp=(F^\perp)^\perp\).
  \end{proof}

  \subsubsection{Sous-espaces vectoriels othogonaux}

  \begin{defdef}
    Soient \(F\) et \(G\) deux sous-espaces vectoriels de \(E\). On dit que \(F\) et \(G\) sont orthogonaux et on note \(F \perp G\), si et seulement si
    \begin{equation}
      \forall x \in F \ \forall y \in G \quad x \perp y.
    \end{equation}
  \end{defdef}
  \begin{prop}
    Soit \(F\) un sous-espace vectoriel de \(E\). Alors
    \begin{enumerate}
      \item \(F \perp F^\perp\);
      \item pour tout sous-espace vectoriel \(G\) de \(E\),
        \begin{equation}
          G \perp F \iff G \subset F^\perp.
        \end{equation}
        \(F^\perp\) est le plus grand sous-espace vectoriel (au sens de l'inclusion) qui est orthogonal à \(F\).
    \end{enumerate}
  \end{prop}
  \begin{proof}
    \begin{enumerate}
      \item Évident par définitin de \(F^\perp\);
      \item Si \(G \perp F^\perp\) soit alors \(x \in G\). Pour tout \(y \in F\), on a \(x \perp y\) (car \(G \perp F\)) donc \(x \in F^\perp\), par définition de \(F^\perp\).
    \end{enumerate}
  \end{proof}
  \begin{prop}
    Soient \(F_1\) et \(F_2\) deux sous-espace vectoriels de \(E\), alors
    \begin{equation}
      F_1 \subset F_2 \implies F_2^\perp \subset F_1^\perp.
    \end{equation}
  \end{prop}
  \begin{proof}
    Soit \(x \in F_2^\perp\), alors pour tout \(y \in F_1\), \(y \in F_2\) donc \(x \perp y\). Alors \(x \in F_1^\perp\).
  \end{proof}
  \begin{prop}
    pour tout sous-espaces vectoriels \(F\) et \(G\) de \(E\), on a
    \begin{equation}
      F \perp G \implies F \cap G = \{0\}.
    \end{equation}
  \end{prop}
  \begin{proof}
    Si \(x \in F \cap G\), alors \(x (\in F) \perp x (\in G)\) donc \(x=0\).  \(F \cap G \subset \{0\}\). L'autre inclusion est triviale puisque \(F \cap G\) est un sous-espace vectoriel.
  \end{proof}
  \begin{cor}
    Pour tout sous-espace vectoriel \(F\) de \(E\), \(F \cap F^\perp =\{0\}\).
  \end{cor}

  \subsubsection{Orthogonalité et supplémentaire}
  \label{subsec:orthetsupp}

  Soit \(F\) un sous-espace vectoriel de \(E\). On sait que~:
  \begin{itemize}
    \item \(F\) et \(F^\perp\) sont en somme directe mais à priori \(F \oplus F^\perp \subset E\);
    \item \(F \subset (F^\perp)^\perp\).
  \end{itemize}

  \begin{theo}\label{theo:orthetsupp}
    Soient \(F\) et \(G\) deux sous-espaces vectoriels de \(E\). On suppose qu'ils sont supplémentaires dans \(E\). Il y a équivalence entre les assertions suivantes~:
    \begin{enumerate}
      \item \(F \perp G\);
      \item \(G = F^\perp\);
      \item \(F = G^\perp\).
    \end{enumerate}
    Auquel cas \(E = F \oplus G = F \oplus F^\perp\).
  \end{theo}
  \begin{proof}
    \(1 \implies 2\) D'après la proposition précédente, on a déjà \(G \subset F^\perp\), montrons l'autre inclusion~: soit \(x \in F^{\perp}\). Comme \(E=F\oplus G\) alors il existe un unique couple \((x_1,x_2) \in F \times G\) tel que \(x=x_1+x_2\). Alors \(0=\prodscal{x}{x_1}=\prodscal{x_1+x_2}{x_1}\). Comme \(x_2 \in G\) et \(x_1 \in F\), et \(F \perp G\) on a \(\prodscal{x_1}{x_2}=0\). Du coup \(\norme{x_1}^2=0\) donc \(x_1=0\). Finalement \(x=x_2 \in G\). Alors \(F^\perp \subset G\). Par double inclusion on a \(G=F^\perp\).

    \(1 \implies 3\) C'est la même démonstration en échangeant \(F\) et \(G\).

    \(2 \implies 1\) et \(3 \implies 1\) C'est clair.
  \end{proof}

  \begin{corth}[Supplémentaire orthogonal]
    \label{corth:supportho}
    Pour tout sous-espace vectoriel \(F\) de \(E\), il existe \emph{au plus} un sous-espace vectoriel \(G\) tel que \(\begin{cases} E = F \oplus G \\ F \perp G \end{cases}\). S'il existe alors \(G=F^\perp\). On parle de supplémentaire orthogonal.
  \end{corth}
  \begin{corth}
    Soit \(F\) un sous-espace vectoriel de \(E\). Si \(E=F \oplus F^\perp\) alors \(F=(F^\perp)^\perp\).
  \end{corth}
  \begin{proof}
    On applique le théorème avec \(G=F^\perp\) : \(F=G^\perp = (F^\perp)^\perp\).
  \end{proof}

  \section{Espace vectoriel euclidien}

  \subsection{Définition d'un espace vectoriel euclidien}

  \begin{defdef}
    On appelle espace vectoriel euclidien tout couple \((E,\varphi)\) où \(E\) est un \(\R\)-espace vectoriel de dimension finie non nulle, et \(\varphi\) un produit scalaire sur \(E\).
  \end{defdef}

  \emph{Remarque~:} Si \(F\) est un sous-espace vectoriel non nul de \(E\), on peut montrer que \(\varphi_F=\varphi_{|F\times F}\) est un produit scalaire sur \(F\). Alors \((F,\varphi_F)\) est un espace vectoriel euclidien.

  \subsection{Supplémentaire orthogonal d'un sous-espace vectoriel euclidien}

  Soit \((E,\varphi)\) un espace vectoriel euclidien.

  \begin{theo}
    Soit \(F\) un sous-espace vectoriel de \(E\). Alors~:
    \begin{itemize}
      \item \(E=F\oplus F^\perp\);
      \item \(\Dim F^\perp = \Dim E - \Dim F\);
      \item \(F=(F^\perp)^\perp\).
    \end{itemize}
    On dit que \(F^\perp\) est le supplémentaire orthogonal de \(F\) (l'unicité a été prouvé à la sous-section~
    \ref{subsec:orthetsupp}). 
  \end{theo}
  \emph{Ne pas confondre l'unicité du supplémentaire orthogonal avec l'unicité du supplémentaire en général. En général un sous-espace vectoriel admet une infinité de supplémentaire.}
  \begin{proof}
    \emph{Cas 1~:} C'est le cas facile où \(F=\{0\}\), alors \(F^\perp =E\) et alors les trois points sont vérifiés.

    \emph{Cas 2~:} On note \(\Dim F=p \in \intervalleentier{1}{n}\) avec \(n = \Dim E\). Soit \((f_1, \ldots, f_p)\) une base de \(F\). On a montré que \(F \oplus F^\perp \subset E\), alors \(\Dim F^\perp \leqslant n-p\). Soit l'application
    \begin{equation}
      \fonction{u}{E}{\R^p}{x}{(\prodscal{x}{f_1}, \ldots, \prodscal{x}{f_p})}.
    \end{equation}
    L'application \(u\) est linéaire, grâce à la linéarité à gauche du produit scalaire. Montrons que \(\Ker(u)=F^\perp\)~:
    \begin{itemize}
      \item Soit \(x \in \Ker(u)\). Pour tout \(i \in \intervalleentier{1}{p}\), on a \(\prodscal{x}{f_i}=0\). Pour tout \(y \in F\), il existe un unique \(p\)-uplet de scalaire \((\alpha_1, \ldots, \alpha_p)\) tel que \(y = \sum_{k=1}\alpha_k f_k\). Alors
        \begin{align}
          \prodscal{x}{y} &= \sum_{k=1}\alpha_k \prodscal{x}{f_k} && \text{linéarité} \\
                          &=\sum_{k=1}\alpha_k 0 && x \in \Ker(u) \\
                          &=0,
        \end{align}
        alors \(x \in F^\perp\). Alors \(\Ker(u) \subset F^\perp\).
      \item Soit \(x \in F^\perp\). Pour tout \(i \in \intervalleentier{1}{p}\), on a \(x\perp f_i\). Donc \(\prodscal{x}{f_i}=0\), et alors \(u(x)=0\). Finalement \(x \in \Ker(u)\). \(F^\perp \subset \Ker(u)\).
    \end{itemize}
    Par double inclusion \(F^\perp = \Ker(u)\).

    En appliquant le théorème du rang à l'application \(u\), on obtient
    \begin{equation}
      \Dim E = \rg(u) + \Dim \Ker(u),
    \end{equation}
    c'est-à-dire
    \begin{equation}
      \Dim F^\perp = n-\rg(u).
    \end{equation}
    Comme \(\Image(u) \subset \R^p\), on a bien \(\rg(u) \leqslant p\) et donc \(\Dim F^\perp \geqslant n-p\). Avec l'inégalité inverse vue plus haut on obtient bien \(\Dim F^\perp = n-p\).

    Finalement \(F \oplus F^\perp \subset E\) et
    \begin{equation}
      \Dim F \oplus F^\perp = \Dim F + \Dim F^\perp = p+n-p=n=\Dim E
    \end{equation}
    nous donne que \(F \oplus F^\perp = E\).
  \end{proof}

  \subsection{Bases orthogonales et bases orthonormales d'un espace vectoriel euclidien}

  \subsubsection{Définition}

  Soient \((E,\varphi)\) un espace vectoriel euclidien et \(\B\) une famille finie de vecteurs de \(E\).
  \begin{defdef}
    \(\B\) est une base orthogonale (BOG) de \(E\) si et seulemet si \(\B\) est une base de \(E\) et si \(\B\) est une famille orthogonale de \(E\).

    \(\B\) est une base orthonormale (BON) de \(E\) si et seulemet si \(\B\) est une base de \(E\) et si \(\B\) est une famille orthonormale de \(E\).
  \end{defdef}

  \emph{Remarque~:} Si \(E\) est une droite vectorielle, alors \(\B=(x)\) est une BOG si et seulement si \(x\) est non nul et c'est une BON si et seulement si \(\norme{x}=1\).

  \subsubsection{Caractérisation}

  Soient \((E,\varphi)\) un espace vectoriel euclidien de dimension \(n\in \N^*\) et \(\B=(e_i)_{i \in \intervalleentier{1}{n}} \in E^n\).

  \begin{prop}
    \(\B\) est une base orthogonale de \(E\) si et seulement si~:
    \begin{itemize}
      \item pour tout \(i \in \intervalleentier{1}{n}\), \(e_i \neq 0\);
      \item pour tout \((i,j)\in \intervalleentier{1}{n}^2\) tel que \(i \neq j\) on a \(e_i \perp e_j\).
    \end{itemize}
  \end{prop}
  \begin{proof}
    \(\implies\) C'est évident : le fait que \(\B\) est une base implique que tous les vecteurs de \(\B\) sont non nuls, et le fait que la famille soit orthogonale implique trivialement l'orthogonalité des vecteurs deux à deux.

    \(\impliedby\) Le deuxième point implique que la famille \(\B\) est orthogonale. De plus c'est une famille orthogonale dont aucun des vecteurs n'est nul donc \(\B\) est une famille libre. C'est une famille libre de cardinal \(n=\Dim E\) donc c'est une base.
  \end{proof}
  %
  \begin{prop}
    \(\B\) est une base orthonormale de \(E\) si et seulement si~:
    \begin{itemize}
      \item pour tout \(i \in \intervalleentier{1}{n}\), \(\norme{e_i}=1\);
      \item pour tout \((i,j)\in \intervalleentier{1}{n}^2\) tel que \(i \neq j\) on a \(e_i \perp e_j\).
    \end{itemize}
    C'est-à-dire
    \begin{equation}
      \B \text{~est une BON} \iff \forall (i,j)\in \intervalleentier{1}{n}^2 \quad \prodscal{e_i}{e_j}=\delta_{ij}.
    \end{equation}
  \end{prop}
  \begin{proof}
    \(\implies\) C'est la définition de la BON.

    \(\impliedby\) D'après la proposition précédente, \(\B\) est une BOG\@. De plus tous ces vecteurs sont unitaires, donc c'est une BON\@.
  \end{proof}
  \begin{prop}
    Soit \(\B=(e_i)_{1 \leqslant i \leqslant n}\) une BOG de \(E\). Alors \(\B'=\left(\frac{e_i}{\norme{e_i}}\right)_{i \in \intervalleentier{1}{n}}=(b_i)_{i \in \intervalleentier{1}{n}}\) est une BON de \(E\).
  \end{prop}
  \begin{proof}
    Déjà \(\B' \subset E\) et \(\Card \B' = n = \Dim E\). De plus
    \begin{equation}
      \forall (i,j) \in \intervalleentier{1}{n}^2 \quad \prodscal{b_i}{b_j}=\frac{\prodscal{e_i}{e_j}}{\norme{e_i}\norme{e_j}} = \delta_{ij}.
    \end{equation}
    Par caractérisation, \(\B\) est une BON de \(E\).
  \end{proof}

  \emph{Exemple~:} La base canonique de \(\R^n\) est une BON de \(\R^n\) munique du produit scalaire canonique.

  \subsection{Existence de bases orthonormales dans un espace euclidien}

  \begin{theo}
    Tout espace vectoriel euclidien admet au moins une base orthonormale.
  \end{theo}
  \begin{proof}
    On prouve par récurrence sur \(n \in \N^{*}\) l'assertion \(\P(n)\) ``Si \(E\) est un espace vectoriel euclidien de dimension \(n\), alors \(E\) admet une base orthonormale.''

    \emph{Initialisation~:} Soit \(E\) un espace vectoriel euclidien de dimension \(1\). Soit \(x \in E\setminus\{0\}\), alors \(\left(\frac{x}{\norme{x}}\right)\) est une bas orthonormale de \(E\). \(\P(1)\) est vraie.

    \emph{Hérédité~:} Soit un naturel \(n\) non nul. Supposons que \(\P(n)\) est vraie, et démontrons \(\P(n+1)\). Soit \(E\) un espace vectoriel euclidien de dimension \(n+1\). Soit \(x \in E\setminus\{0\}\) et posons \(F = \VectEngendre(x)\).

    \(F^\perp\) est un sous-espace vectoriel de \(E\) de dimension \(n\). Muni du produit scalaire induit par celui de \(E\), \(F^\perp\) est un sous espace vectoriel euclidien de dimension \(n\). Appliquons-lui l'hypothèse de récurrence : Il existe une base orthonormale \(\B'=(b_1, \ldots, b_n)\) de \(F^\perp\). Soit \(\B=\B'\cup\{b_{n+1}\}\) avec \(b_{n+1}=\left(\frac{x}{\norme{x}}\right)\). On vérifie que \(\B\) est une base orthonormale de \(E\)~:
    \begin{itemize}
      \item \(\Card(\B)=n+1=\Dim(E)\);
      \item Pour tout \((i,j) \in \intervalleentier{1}{n+1}^2\) on a~:
        \begin{equation}
          \prodscal{b_i}{b_j} = \begin{cases}
            \delta_{ij} & \text{si~} i \leqslant n \text{~et~} j \leqslant n \\
                        & \text{~car~} \B' \text{~est une BON} \\
            0 = \delta_{ij} & \text{si~} (i \leqslant n, \ j=n+1) \text{~ou~} (j \leqslant n, \ i=n+1) \\
                            &  \text{~car~} F \perp F^{\perp} \\
            1 = \delta_{ij} & \text{si~} i=j=n+1 \text{~car~} \prodscal{b_{n+1}}{b_{n+1}}=1
          \end{cases}
        \end{equation}
    \end{itemize}
    Donc \(\B\) est une base orthonormale de \(E\).

    \emph{Conclusion~:} Le théorème de récurrence permet de conclure que l'assertion \(\P(n)\) est vraie pour tout naturel \(n\) non nul.
  \end{proof}
  \begin{theo}
    Toute famille orthonormale d'un espace vectoriel euclidien \(E\) peut être complétée en une base orthonormale de \(E\).
  \end{theo}
  \begin{proof}
    Soit \((E,\varphi)\) un espace vectoriel euclidien de dimension \(n \geqslant 1\). Soit \(\F\) une famille orthonormale de \(E\). \(\F\) est donc une famille libre de \(E\). Soit \(p \leqslant n\) le cardinal de \(\F\), et notons \(\F=(f_1, \ldots, f_p)\). Deux cas se présentent~: Si \(p=n\) alors \(\F\) est déjà une base orthonormale.

    Sinon, on note \(F=\VectEngendre(\F)\), et \(\dim F^{\perp} = n-p\geqslant 1\). Muni du produit scalaire induit par celui de \(E\), \(F^\perp\) est un espace vectoriel euclidien. D'après le théorème précédent, \(F^\perp\) admet une base orthonormale notée \(\F'=(f_{p+1}, \ldots, f_{n})\). Soit \(\B=\F\cup\F'\). Montrons que \(\B\) est une base orthonormale de \(E\)~:
    \begin{itemize}
      \item \(\Card(\B)=n=\Dim(E)\);
      \item Pour tout \((i,j) \in \intervalleentier{1}{n}^2\) on a
        \begin{equation}
          \prodscal{f_i}{f_j} = \begin{cases} \delta_{ij} & \text{si~} (i \leqslant p, \ j \leqslant p) \text{~ou~} (i \geqslant p+1, \ j \geqslant p+1) \\
        & \text{~car~} \F, \F'\text{~sont des~} BON \\
            0=\delta_{ij} & \text{~si~} (i \leqslant p, \ j \leqslant p) \text{~ou~} (i \geqslant p+1, \ j \geqslant p+1)\\
                          &  \text{~car~} F \perp F^\perp 
          \end{cases}
          \end{equation}
          Donc \(\B\) est une base orthonormale de \(E\).
      \end{itemize}
    \end{proof}
    \begin{theo}
      Soit \(E\) un espace vectoriel euclidien de dimension \(n \geqslant 1\). Soit \(\B=(b_1, \ldots, b_n)\) une base orthonormale de \(E\). Soit \(p \in \intervalleentier{0}{n}\). On note \(\B_1 = (b_i)_{1 \leqslant i \leqslant p}\) (\(\B_1 = \emptyset\) si \(p=0\)) et \(\B_2 = (b_i)_{p+1 \leqslant i \leqslant n}\) (\(\B_2 = \emptyset\) si \(p=n\)). Soient \(F_1=\VectEngendre(\B_1)\) et \(F_2=\VectEngendre(\B_2)\). Alors
      \begin{equation}
        \begin{cases}
          E = F_1 \oplus F_2 \\
          F_2 = F_1^\perp
        \end{cases}.
      \end{equation}
      De plus \(\B_1\) et \(\B_2\) sont des bases orthonormales respectives de \(F_1\) et \(F_2\).
    \end{theo}
    \begin{proof}
      On a déjà vu au chapitre~
      \ref{chap:dimensionfinie} (théorème~
      \ref{theo:theosuppdimfinie}) que \(F_1\) et \(F_2\) sont supplémentaires dans \(E\). Montrons l'orthogonalité~:

      Soit \(x \in F_1\) et \(y \in F_2\). Il existe \((\alpha_i)_{1 \leqslant i \leqslant p} \in \R^p\) et \((\beta_i)_{p+1 \leqslant i \leqslant n}\in \R^{n-p}\) telles que \(x = \sum_{i=1}^p \alpha_i b_i\) et \(y = \sum_{i=p+1}^n \beta_i b_i\). Leur produit scalaire vaut
      \begin{align}
        \prodscal{x}{y} &= \prodscal{\sum_{i=1}^p \alpha_i b_i}{\sum_{j=p+1}^n \beta_j b_j}\\
                        &= \sum_{i=1}^p \alpha_i \sum_{j=p+1}^n \beta_j \underbrace{\prodscal{b_i}{b_j}}_{=0 \text{~car~} i\neq j}\\
                        &=0.
      \end{align}
      Donc \(F_1 \perp F_2\), et puisque \(E = F_1 \oplus F_2\), alors \(F_2 = F_1^\perp\), d'après le corollaire~
      \ref{corth:supportho}.

      \(\B_1\) et \(\B_2\) vérifient toutes les deux~:
      \begin{equation}
        \forall (i,j) \in \intervalleentier{1}{n}^2 \quad \prodscal{b_i}{b_j}=\delta_{ij}.
      \end{equation}
      De plus \(\B_1\) et \(\B_2\) sont des bases de \(F_1\) et \(F_2\) (libres car extraites de \(\B\) et génératrices par définition de \(F_1\) et\(F _2\)). Alors \(\B_1\) et \(\B_2\) sont des bases orthonormales respectives de \(F_1\) et \(F_2\).
    \end{proof}

    \subsection{Construction d'une base orthonormale par le procédé de Gram-Schmidt}

    \begin{theo}
      Soit \((E,\varphi)\) un espace vecoriel euclidien de dimension \(n \geqslant 1\). Soit \(\E=(e_1, \ldots, e_n)\) une base quelconque de \(E\). Alors il existe de façon unique une famille de réels \((\alpha_{ij})_{1 \leqslant i<j \leqslant n}\) telle que la famille \((b_j)_{1 \leqslant j \leqslant n} \in E^n\) vérifiant
      \begin{equation} \Sigma
        \begin{cases}
          b_1 & = e_1 \\
          b_2 & = \alpha_{12}b_1 +e_2 \\
          \vdots & \\
          b_j & = \sum_{i=1}^{j-1} \alpha_{ij}b_i  + e_j \\
          \vdots & \\
          b_n & = \sum_{i=1}^{n-1} \alpha_{in}b_i  + e_n
        \end{cases}
      \end{equation}
      soit une base orthogonale de \(E\). De plus
      \begin{equation}
        \forall (i,j) \in \intervalleentier{1}{n}^2 \ 1 \leqslant i < j \leqslant n \quad \alpha_{ij} = -\frac{\prodscal{b_i}{e_j}}{\norme{b_i}^2}.
      \end{equation}
      Il reste à normaliser cette base~: la famille \(\left(\frac{b_j}{\norme{b_j}}\right)_{1 \leqslant j \leqslant n}\) est une base orthonormale de \(E\).
    \end{theo}
    \begin{proof}[Démonstration par récurrence]
      Pour tout naturel \(n\) non nul on pose \(\P(n)\) ``pour tout espace vectoriel euclidien de dimension \(n\), toute base \(\E\), \ldots''

      \emph{Initialisation~:} Soit \(E\) un espace vectoriel euclidien de dimension \(1\). Soit \(\E=(e_1)\) une base de \(E\), et elle est orthogonale. \(\P(1)\) est vraie.

      \emph{Hérédité~:} Soit \(n \in \N^*\), et supposons \(\P(n)\). Soit \(E\) un espace vectoriel euclidien de dimension \(n+1\), \(\E=(e_1, \ldots, e_{n+1})\) une base de \(\E\). Soient \(\E'=\E\setminus\{e_{n+1}\}\) et \(E'=\VectEngendre(\E')\) muni du produit scalaire induit. \(E'\) est un espace vectoriel euclidien de dimension \(n\). Par hypothèse de récurrence, il existe, de façon unique, une famille de réels \((\alpha_{ij})_{1 \leqslant i<j \leqslant n}\) telle que la famille \((b_j)_{1 \leqslant j \leqslant n} \in E^n\) vérifiant \(\Sigma\) formant une base orthogonale de \(E'\). 

      Il reste à prouver l'existence et l'unicité de \((\alpha_{i n+1})_{1 \leqslant i \leqslant n} \in \R^n\) telle que si \(b_{n+1} = \sum_{i=1}^{n} \alpha_{i n+1}b_i  + e_{n+1}\) alors \((b_j)_{1 \leqslant j \leqslant n+1}\) est une base orthogonale de \(E\).

      \emph{Unicité (Analyse)~:} Si les \(\alpha_{i n+1}\) existent alors pour tout \(j \in \intervalleentier{1}{n}\), on a
      \begin{align}
        0 &= \prodscal{b_{n+1}}{b_j} \\
        0 &= \prodscal{\sum_{i=1}^{n} \alpha_{i n+1}b_i  + e_{n+1}}{b_j}\\
        0 &= \sum_{i=1}^{n} \alpha_{i n+1} \prodscal{b_i}{b_j} + \prodscal{e_{n+1}}{b_j} && \text{linéarité à gauche}\\
        0 &= \alpha_{j n+1} \prodscal{b_j}{b_j} + \prodscal{e_{n+1}}{b_j} && (b_j)_{1 \leqslant j \leqslant n} \text{~est orthogonale}\\
        \alpha_{j n+1} &= -\frac{\prodscal{e_{n+1}}{b_j}}{\norme{b_j}^2}. && b_j \neq 0
      \end{align}
      On a montré l'unicité sous réserve d'existence et on a trouvé la seule valeur possible des \(\alpha_{i n+1}\).

      \emph{Existence (Synthèse)~:} Pour tout \(i \in \intervalleentier{1}{n}\), on pose \(\alpha_{i n+1} = -\frac{\prodscal{e_{n+1}}{b_i}}{\norme{b_i}^2}\) et \(b_{n+1}=\sum_{i=1}^{n} \alpha_{i n+1}b_i  + e_{n+1}\). 

      Montrons que \((b_j)_{1 \leqslant j \leqslant n+1}\) est une base orthogonale de \(E\)~:
      \begin{itemize}
        \item Pour tout \((i,j) \in \intervalleentier{1}{n}^2\) si \(i \neq j\) alors \(\prodscal{b_i}{b_j}=0\) car \((b_j)_{1 \leqslant j \leqslant n}\) est une base orthogonale de \(E\). Ensuite
          \begin{align}
            \prodscal{b_i}{b_{n+1}} &= \prodscal{b_i}{\sum_{i=1}^{n} \alpha_{i n+1}b_i  + e_{n+1}} \\
                                    & = \sum_{i=1}^{n} \alpha_{i n+1} \prodscal{b_i}{b_j} + \prodscal{b_i}{e_{n+1}} \\
                                    &=\alpha_{i n+1} \norme{b_i}^2 + \prodscal{b_i}{e_{n+1}} \\
                                    &=0.
          \end{align}
          Alors \((b_j)_{1 \leqslant j \leqslant n+1}\)  est une famille orthogonale. Montrons que c'est une base~:
          \begin{itemize}
            \item Si \(i \leqslant n\), alors \(b_i \neq 0\) car \((b_j)_{1 \leqslant j \leqslant n}\) est une base orthogonale de \(E'\);
            \item Si on avait \(b_{n+1}=0\), on aurait \(e_{n+1}= -\sum_{i=1}^{n} \alpha_{i n+1}b_i\). C'est-à-dire \(e_{n+1} \in \VectEngendre(b_1, \ldots, b_n) = \VectEngendre(e_1, \ldots, e_n)\). Or c'est impossible car \((e_1, \ldots, e_{n+1})\) est une base. Donc \(b_{n+1} \neq 0\).
          \end{itemize}
          \((b_j)_{1 \leqslant j \leqslant n+1}\)  est une famille orthogonale dont tous les vecteurs sont non nuls, elle est donc libre. Finalement c'est une base car \(\Card(b_j)_{1 \leqslant j \leqslant n+1}=n+1=\Dim E\).
      \end{itemize}
      Alors \(\P(n+1)\) est vraie.

      \emph{Conclusion~:} Le théorème de récurrence nous permet de conclure en écrivant que l'assertion \(\P(n)\) est vraie pour tout naturel \(n\) non nul.
    \end{proof}

    \emph{Remarques~:}
    \begin{enumerate}
      \item La démonstration par récurrence fournit un procédé pour déterminer la base \((b_1, \ldots, b_n)\), appelé procédé d'orthogonalisation ou procédé de Gram Schmidt;
      \item La matrice de passage \(\P_{\B,\E}\) de la base \(\E\) à la base \(\B\) est triangulaire supérieure avec que des \(1\) sur la diagonale, idem pour \(\P_{\E,\B}=\P_{\B,\E}^{-1}\);
      \item Pour tout naturel \(p \in \intervalleentier{1}{n}\), \(\VectEngendre(e_1, \ldots, e_p)=\VectEngendre(b_1, \ldots, b_p)\).
    \end{enumerate}

    \subsection{Expressions analytiques dans une base orthonormée donnée}

    Soit \((E,\varphi)\) un espace vectoriel euclidien de dimension \(n \geqslant 1\). Soit \(\B=(b_1, \ldots, b_n)\) une base orthonormée de \(E\).

    \subsubsection{Coordonnées d'un vecteur}

    Soit \(x \in E\), \(\B\) une base de \(E\). Il existe un unitque \(n\)-uplet \((x_i)_{1 \leqslant i \leqslant n}\in\R^n\) tel que \(x = \sum_{i=1}^n x_ib_i\). Pour tout \(j \in \intervalleentier{1}{n}\), on a
    \begin{align}
      \prodscal{x}{b_j} &=\sum_{i=1}^n x_i\prodscal{b_i}{b_j} && \text{linéarité} \\
                        &=x_j. && \B \text{~est une BON}
    \end{align}

    Alors pour tout \(x \in E\), on a \(x = \sum_{i=1}^n \prodscal{x}{b_i}b_i\).

    \subsubsection{Produit scalaire}

    Soient \(x\) et \(y\) des vecteurs de \(E\). Il existe deux uniques familles de scalaire \((x_i)_{1 \leqslant i \leqslant n}\in\R^n\) et \((y_i)_{1 \leqslant i \leqslant n}\in\R^n\) telles que
    \begin{align}
      x &= \sum_{i=1}^n x_ib_i \\
      y &= \sum_{i=1}^n y_ib_i.
    \end{align}
    Ainsi le produit scalaire s'écrit
    \begin{align}
      \prodscal{x}{y} &=\prodscal{\sum_{i=1}^n x_ib_i}{\sum_{j=1}^n y_jb_j} \\
                      &=\sum_{i=1}^n  \sum_{j=1}^n x_i y_j \prodscal{b_i}{b_j} && \text{bilinéarité} \\
                      &=\sum_{i=1}^n  x_i y_i. && \B \text{~est une BON}
    \end{align}

    Finalement pour tout \((x,y) \in E^2\), on a \(\prodscal{x}{y} = \sum_{i=1}^n \prodscal{x}{b_i}\prodscal{y}{b_i}\).

    \emph{Remarque~:} Si on note \(X=\Mat_{\B}(x) \in \Mnp{n}{1}{\R}\) et \(Y=\Mat_{\B}(y) \in \Mnp{n}{1}{\R}\), alors \(\prodscal{x}{y} = X^{\top}Y\).

    \subsubsection{Norme euclidienne}

    Soit \(x \in E\), il existe un unique \(n\)-uplet de scalaires \((\lambda_i)_{i \in \intervalleentier{1}{n}}\) tel que \(x = \sum_{i=1}^n x_i b_i\). On note \(X = \Mat_{\B}(x)\). Alors
    \begin{gather}
      \norme{x} = \sqrt{\sum_{i=1}^n x_i^2}=\sqrt{\sum_{i=1}^n \prodscal{x}{b_i}^2} \\
      \norme{x}^2 = X^\top X
    \end{gather}

    \subsection{Isomorphisme entre un espace euclidien et \(\R^n\)}

    \begin{theo}
      Soit \((E,\varphi)\) un espace vectoriel euclidiende dimension \(n \geqslant 1\). Soit \(\B\) une base orthonormale de \(E\). On note \(\E_c\) la base canonique de \(\R^n\) muni du produit scalaire canconique.

      Il existe un unique isomorphisme \(u \in \Isom{E}{\R^n}\) tel que \(u(\B)=\E_c\). De plus \(u\) conserve le produit scalaire~:
      \begin{equation}
        \forall (x,y) \in E^2 \quad \prodscal{u(x)}{u(y)}=\varphi(x,y)=\prodscal{x}{y}.
      \end{equation}
    \end{theo}
    \begin{proof}
      \(\B\) est une base de \(E\) et \(\Dim(E)=n\), alors il existe un unique \(u \in \Lin{E}{\R^n}\) tel que \(u(\B)=\E_c\). De plus \(\E_c\) est une base de \(\R^n\), donc \(u\) est bijective, c'est un isomorphisme de \(E\) sur \(\R^n\).

      Pour tout couple \((x,y) \in E^2\), il existe deux uniques \(n\)-uplets \((x_i)_{i \in \intervalleentier{1}{n}}\in \R^n\) et \((y_i)_{i \in \intervalleentier{1}{n}}\in \R^n\) tels que
      \begin{equation}
        x = \sum_{i=1}^n x_i b_i, \quad y = \sum_{i=1}^n y_i b_i.
      \end{equation}
      Alors
      \begin{align}
        \prodscal{u(x)}{u(y)} &=\prodscal{\sum_{i=1}^n x_i u(b_i)}{\sum_{j=1}^n y_j u(b_j)} && u \in \Lin{E}{\R^n} \\
                              &=\prodscal{\sum_{i=1}^n x_i e_i}{\sum_{j=1}^n y_j e_j} && \text{définition de } u\\
                              &=\sum_{i=1}^n \sum_{j=1}^n x_iy_j \prodscal{e_i}{e_j} && \text{bilinéarité} \\
                              &=\sum_{i=1}^n x_i y_i && \E_c \text{~est une  BOG} \\
                              &=\varphi(x,y)=\prodscal{x}{y}.
      \end{align}
    \end{proof}
    \begin{theo}
      Soit \(E\) un espace vectoriel euclidien de dimension \(n \geqslant 1\). Soit \(\B\) une base de \(E\). Il existe un unique produit scalaire \(\varphi\) sur \(E\) tel que la base \(\B\) soit une base orthonormale de \(E\) pour le produit scalaire \(\varphi\).
    \end{theo}
    \begin{proof}[Analyse \& Unicité]
      Supposons avoir construit le produit scalaire \(\varphi\).

      On peut lui appliquer le théorème précédent, alors pour tout \((x,y) \in E^2\), on a \(\varphi(x,y)=\prodscal{u(x)}{u(y)}\). De plus \(u\) ne dépend que de la base \(\B\) et pas du produit scalaire \(\varphi\).

      Alors \(\varphi\) est défini en fonction unique de \(u\), et donc de \(\B\).
    \end{proof}
    \begin{proof}[Synthèse \& Existence]
      On sait grâce au htéorème précédent qu'il existe un unique isomorphisme \(u \in \Isom{E}{\R^n}\) tel que \(u(\B)=\E_c\). On définit l'application
      \begin{equation}
        \fonction{\varphi}{E^2}{\R}{(x,y)}{\prodscal{u(x)}{u(y)}}.
      \end{equation}
      L'application \(\varphi\) est symétrique puisque \(\prodscal{.}{.}\) l'est. Pour tout réel \(\lambda\) et tout triplet \((x,x',y) \in E^3\) on a
      \begin{align}
        \varphi(\lambda x+x',y) &= \prodscal{u(\lambda x+x')}{u(y)} \\
                                &=\lambda \prodscal{u(x)}{u(y)}+ \prodscal{u(x')}{u(y)} && \text{linéarité de } u \text{~et~} \prodscal{.}{.} \\
                                &=\lambda \varphi(x,y)+ \varphi(x,y').
      \end{align}
      \(\varphi\) est linéaire à gauche. Elle est aussi linéaire à droite et symétrique.

      Soit \(x \in E\), \(\varphi(x,x)=\norme{u(x)}^2 \geqslant 0\) et \(\varphi(x,x)=0 \iff u(x)=0 \iff x=0\) car \(u\) est bijective (donc injective). L'application \(\varphi\) est définie positive.

      Au final, \(\varphi\) est un produit scalaire.

      Il reste à vérifier que \(\B\) est une base orthonormale pour \(\varphi\). Pour tout \((i,j) \in \intervalleentier{1}{n}^2\), on a
      \begin{align}
        \varphi(b_i,b_j) &= \prodscal{u(b_i)}{u(b_j)} \\
                         &= \prodscal{e_i}{e_j} && u(\B)=\E_c\\
                         &=\delta_{ij}. && \E_c \text{~est une BON}
      \end{align}
      Alors \(\B\) est une base orthonormale.
    \end{proof}

    \section{Projecteurs orthogonaux}

    Soit \((E, \varphi)\) un espace vectoriel euclidien.

    \subsection{Notion de projecteur orthogonal}

    Soit \(p \in \Endo{E}\) un projecteur. Alors \(E = \Image(p)\oplus \Ker(p)\). D'après le théorème~
    \ref{theo:orthetsupp}, il y a équivalence entre
    \begin{enumerate}
      \item \(\Image(p) \perp \Ker(p)\);
      \item \(\Image(p) = \Ker(p)^\perp\);
      \item \(\Ker(p) = \Image(p)^\perp\).
    \end{enumerate}
    Lorsqu'une de ces conditions est vérifiée, on dit que \(p\) est un projecteur orthogonal.

    \emph{Vocabulaire~:} Soit \(F\) un sous-espace vectoriel de \(E\), \(E = F \oplus F^\perp\). Le projecteur sur \(F\) parallélement à \(F^\perp\) et noté \(p_F\) est appelé projecteur orthogonal de \(E\) sur \(F\).

    \begin{theo}
      Soit \(p\) ine application de \(E\) dans \(E\). Il y a équivalence entre les assertions suivantes~:
      \begin{enumerate}
        \item \(p\) est un projecteur orthognal de \(E\);
        \item \(p \in \Endo{E}\), \(p \circ p =p\), et pour tout \((x,y) \in E^2\) on a \(\prodscal{x}{p(y)}=\prodscal{p(x)}{y}\).
      \end{enumerate}
    \end{theo}
    \begin{proof}
      \(1 \implies 2\). Comme \(p\) est un projecteur, alors \(p \in \Endo{E}\) et \(p \circ p =p\). Soient \((x,y) \in E^2\) alors
      \begin{align}
        \prodscal{p(x)}{y} &=  \prodscal{p(x)}{y-p(y)+p(y)} \\
                           &= \prodscal{\underbrace{p(x)}_{\in \Image(p)}}{\underbrace{y-p(y)}_{\in \Ker(p)}}+\prodscal{p(x)}{p(y)} \\
                           &=  \prodscal{p(x)}{p(y)} && \Image(p) \perp \Ker(p)
      \end{align}
      De la même manière, par symétrie des rôles de \(x\) et \(y\) on a \(\prodscal{x}{p(y)}=\prodscal{p(x)}{p(y)}\). Donc \(\prodscal{x}{p(y)}=\prodscal{p(x)}{y}\).

      \(2 \implies 1\). \(p \in \Endo{E}\) et \(p \circ p =p\) impliquent que \(p\) est un projecteur de \(E\).

      Soit \(x \in \Ker(p)\) et \(y \in \Image(p)\). Alors
      \begin{align}
        \prodscal{x}{y} &= \prodscal{x}{p(y)} \\
                        &=\prodscal{p(x)}{y} \\
                        &=\prodscal{0}{y}=0.
      \end{align}
      Donc \(\Image(p) \perp \Ker(p)\). Alors le projecteur \(p\) est orthogonal.
    \end{proof}

    \subsection{Expression du projeté orthogonal}

    Soit \(F\) un sous-espace vectoriel de \(E\). Soit \(p_F\) le projecteur orthogonal sur \(F\). On se donne une \emph{base orthogonale} \((f_1, \ldots, f_p)\) de \(F\). On veut obtenir une expression de \(p_F(x)\) pour tout vecteur \(x\) de \(E\).

    Soit \(x \in E\), alors \(p_F(x) \in F\) et
    \begin{align}
      p_F(x) &=\sum_{k=1}^n \prodscal{p_F(x)}{f_k}f_k \\
             &=\sum_{k=1}^n \prodscal{x}{p_F(f_k)}f_k \\
             &=\sum_{k=1}^n \prodscal{x}{f_k}f_k. && f_k \in F=\Image(p_F)
    \end{align}

    \emph{Remarque~:} Pour tout \(x \in E\), \(x = p_F(x) +(x-p_F(x)) = p_F(x) + p_{F^\perp}(x)\). Parfois c'est plus intéressant de calculer \(p_{F^\perp}\), soit parce qu'il est de dimension plus petite, soit parce qu'on connait une de ses bases orthonormales.

    \paragraph{Matrice d'un projecteur orthogonal}
    Soit \(F\) un sous-espace vectoriel de dimension \(p \geqslant 1\). Soit \(p_F\) un projecteur orthogonal sur \(F\). Soit \(\F=(f_1, \ldots, f_p)\) une base orthonormale de \(F\) et \(\B=(b_1, \ldots, b_n)\) une base orthonormale de \(E\).

    Soit \(A=\Mat_\B(p_F)\). Calculons pour tout \(i \in \intervalleentier{1}{n}\) \(p_F(b_i)\)~: Pour tout \(j \in \intervalleentier{1}{n}\), on a
    \begin{equation}
      p_F(b_j)=\sum_{k=1}^p \prodscal{b_j}{f_k}f_k.
    \end{equation}

    Il faut les coordonnées de \(p_F(b_j)\) dans la base \(\B\)~: ce sont les \(\prodscal{p_F(b_j)}{b_i}\).
    \begin{align}
      a_{ij} &= \prodscal{p_F(b_j)}{b_i} \\
             &= \prodscal{\sum_{k=1}^p \prodscal{b_j}{f_k}f_k}{b_i} \\
             &= \sum_{k=1}^p \prodscal{b_j}{f_k} \prodscal{b_i}{f_k}.
    \end{align}

    La matrice \(A\) est symétrique.

    \subsection{Distance d'un point à un sous-espace vectoriel}

    Soit \(F\) un sous-espace vectoriel de \(E\) de dimension \(p \geqslant 1\). Soit \(x \in E\), la partie \(\enstq{d(x,y)}{y \in F}\) est une partie de \(\R\) non vide (car \(F\) est non vide) et est minorée par \(0\).

    Elle admet donc une borne inférieure noté \(d(x,F)=\inf\enstq{d(x,y)}{y \in F}\). C'est la distance du point \(x\) au sous-esapce vectoriel \(F\).

    \begin{theo}
      \begin{enumerate}
        \item Cette borne inférieure est en fait un minimum;
        \item pour tout vecteur \(y \in F\), on a
          \begin{equation}
            d(x,F) = d(x,p_F(x)) = \norme{x-p_F(x)} \leqslant \norme{x-y};    
          \end{equation}
        \item pour tout vecteur \(y \in F\), on a
          \begin{equation}
            d(x,F) = d(x,y) \implies y=p_F(x).
          \end{equation}
          Le minimum est atteint en \(p_F(x)\) seulement.
      \end{enumerate}
    \end{theo}
    \begin{proof}
      Soit \(y \in F\), alors
      \begin{align}
        \norme{x-y}^2 &=\norme{\underbrace{x-p_F(x)}_{\in F^\perp} + \underbrace{p_F(x)-y}_{\in F}}\\
                      &=\norme{x-p_F(x)}^2 + \norme{p_F(x)-y}^2. && \text{Pythagore}
      \end{align}

      Donc \(\norme{x-p_F(x)} \leqslant \norme{x-y}\), et
      \begin{equation}
        \norme{x-y}^2 = \norme{x-p_F(x)}^2 \iff  \norme{p_F(x)-y}=0.
      \end{equation}
      Cela signifie que \(d(x,F)\) est un minimum et qu'il est atteint en \(p_F(x)\) : \(d(x,F)=\norme{x-p_F(x)}\)
    \end{proof}

    \paragraph{Expressions de \(d(x,F)\)}

    \begin{itemize}
      \item
        \begin{gather}
          d(x,F) = \norme{x-p_F(x)} = \norme{p_{F^\perp}(x)} \\
          \norme{x}^2 = \norme{x-p_F(x)+p_F(x)}^2=\norme{x-p_F(x)}^2+\norme{p_F(x)}^2\\
          d(x,F) = \sqrt{\norme{x}^2-\norme{p_F(x)}^2}
        \end{gather}
      \item Soit \(\B=(b_1,\ldots, b_n)\) une base orthonormale de \(E\) et \(\F=(f_1,\ldots, f_p)\) une base orthonormale de \(F\), alors
        \begin{equation}
          d(x,F) = \sqrt{\sum_{i=1}^n\prodscal{x}{b_i}^2-\sum_{k=1}^p\prodscal{x}{f_k}^2}.
        \end{equation}
      \item Il est parfois utile de calculer \(p_{F^\perp}(x)\) plutôt que \(p_{F^\perp}(x)\).
    \end{itemize}

    \subsection{Formes linéaires et hyperplans d'un espace euclidien}

    \subsubsection{Vecteur normal et forme linéaire}

    Soit \((E,\varphi)\) un espace vectoriel euclidien. On note \(E^*=\Lin{E}{\R}\) son dual, c'est-à-dire l'ensemble des formes linéaires de \(E\). Pour tout \(x \in E\), on pose
    \begin{equation}
      \fonction{\varphi_x}{E}{\R}{y}{\prodscal{x}{y}}.
    \end{equation}
    Le produit scalaire est linéaire à droite donc \(\varphi_x \in E^*\). Soit l'application
    \begin{equation}
      \fonction{\psi}{E}{E^*}{x}{\varphi_x}.
    \end{equation}
    L'application \(\psi\) est bien définie car pour tout vecteur \(x \in E\), \(\varphi_x \in E^*\). Montrons que \(\psi\) est linéaire~: Soit \((x,x') \in E^2\) et \(\lambda \in \R\), alors \(\psi(\lambda x+x') = \varphi_{\lambda x+x'}\). Soit \(y \in E\), alors
    \begin{align}
      \psi(\lambda x+x')(y) &= \varphi_{\lambda x+x'}(y) \\
                            &=\prodscal{\lambda x+x'}{y} \\
                            &=\lambda \prodscal{x}{y} + \prodscal{x'}{y} && \text{linéarité à gauche}\\
                            &=\lambda \varphi_x(y)+ \varphi_{x'}(y) \\
                            &=\lambda \psi(x)(y)+\psi(x')(y)\\
                            &=[\lambda \psi(x)+\psi(x')](y).
    \end{align}
    Comme l'égalité est vraie pour tout \(y \in E\), on a \(\psi(\lambda x+x')=\lambda \psi(x)+\psi(x')\), donc \(\psi \in \Lin{E}{E^*}\).

    Trouvons le noyau de \(\psi\).

    Soit \(x \in E\), alors
    \begin{align}
      x \in \Ker(\psi) &\iff \psi(x) = 0_{E^*} \\
                       &\iff \forall y \in E \quad \psi(x)(y)=0_{\R} \\
                       &\iff \forall y \in E \quad \varphi_x(y)=0_{\R} \\
                       &\iff \forall y \in E \quad \prodscal{x}{y}=0_\R \\
                       &\iff x=0_E.
    \end{align}
    Donc \(\Ker \psi = \{0_E\}\). L'application \(\psi\) est injective et \(\Dim E= \Dim E^*\) donc \(\psi\) est bijective. On en déduit le théorème suivant
    \begin{theo}
      L'application
      \begin{equation}
        \fonction{\psi}{E}{E^*}{x}{\fonction{\varphi_x}{E}{\R}{y}{\prodscal{x}{y}}}
      \end{equation}
      est un isomorphisme de \(\R\)-espace vectoriels de \(E\) dans \(E^*\).
    \end{theo}
    \begin{corth}
      \begin{gather}
        \forall f \in E^* \ \exists! a \in E \quad f=\varphi_a \\
        \forall f \in E^* \ \exists! a \in E \ \forall x \in E \quad f(x)=\prodscal{a}{x}.
      \end{gather}
    \end{corth}
    \emph{Vocabulaire~:} Le vecteur \(a\) est le \emph{vecteur normal} de la forme linéaire \(f\).

    \subsubsection{Applications aux hyperplans d'unespace vectoriel euclidien}

    Soit \(H\) un hyperplan de \(E\). Il existe une forme linéaire \(f\) non nulle telle que \(H= \Ker(f)\).

    Pour toute forme linéaire \(g\),
    \begin{equation}
      H = \Ker(g) \iff \exists \lambda \in \R^* \quad g=\lambda f.
    \end{equation}

    Soit \(a\) le vecteur normal de \(f\). \(a\) est non nul parce que \(f\) est non nulle. Soit un vecteur \(x \in E\), alors
    \begin{equation}
      f(x)= \prodscal{a}{x},
    \end{equation}
    donc pour tout \(x \in E\), \((\lambda f)(x) = \prodscal{\lambda a}{x}\). Le vecteur normal de \(\lambda f\) est \(\lambda a\).

    \begin{defdef}
      Soit \(H\) un un hyperplan de \(E\) et une forme linéaire \(f\) non nulle telle que \(H= \Ker(f)\). Soit \(a\) le vecteur normal de \(f\). \(a\) est appelé \emph{un} vecteur normal  de \(H\). Les autres sont les \(\lambda a\), avec \((\lambda \neq 0)\).
    \end{defdef}

    \paragraph{Propriétés des vecteurs normaux}

    Soit \(H\) un hyperplan de \(E\). Il existe une forme linéaire \(f\) non nulle telle que \(H= \Ker(f)\). Soit un vecteur \(x \in E\), alors
    \begin{align}
      x \in H &\iff f(x) = 0 \\
              &\iff \prodscal{x}{a}=0\\
              &\iff a \in H^\perp.
    \end{align}
    De plus \(\Dim H^\perp =1\) et \(a\) est non nul, donc c'est une base de \(H^\perp\). Ainsi \(\left(\frac{a}{\norme{a}}\right)\) est une base orthonormée de \(H^\perp\).

    \paragraph{Conséquences}
    Soit un vecteur \(x \in E\).  Alors
    \begin{gather}
      p_{H^\perp}(x) = \prodscal{x}{\frac{a}{\norme{a}}}\frac{a}{\norme{a}} = \frac{\prodscal{x}{a}}{\prodscal{a}{a}}a \\
      p_H(x) = x-\frac{\prodscal{x}{a}}{\prodscal{a}{a}}a.
    \end{gather}

    La distance de \(x\) à l'hyperplan \(H\) vaut
    \begin{equation}
      d(x,H) = \norme{p_{H^\perp}(x)} = \frac{\abs{\prodscal{x}{a}}}{\norme{a}}.
    \end{equation}

    \subsection{Orientation d'un hyperplan}

    Soit \(E\) un espace vectoriel de dimension finie non nulle \(n\), supposée orienté. Soient \(F\) et \(G\) deux sous-espaces vectoriels orthogonaux et supplémentaires de \(E\)~:
    \begin{equation}
      E = F \oplus G \quad F \perp G.
    \end{equation}
    Soient \(\E_F=(e_1, \ldots, e_p)\) une base de \(F\), \(\E_G=(e_{p+1}, \ldots, e_n)\) une base de \(G\) et \(\E=\E_G \cup \E_F\) une base de \(E\). On suppose que \(G\) est orienté par \(\E_G\), et que \(\E_G\) est une base directe de \(G\). On veut orienter \(F\). Deux possibilités se présentent~:
    \begin{itemize}
      \item Si \(\E\) est une base directe de \(E\), on décide d'orienter \(F\) par \(\E_F\) (\(\E_F\) est une base directe de \(F\));
      \item Si \(\E\) est une base indirecte de \(E\), on décide que \(\E_F\) est une base indirecte de \(F\). On dit que l'on a induit une orientation de \(F\) par l'orientation de \(G\).
    \end{itemize}

    En particulier si \(F=H\), et un hyperplan \(G=\VectEngendre(a)\) avce \(a\) un vecteur normal de \(H\). Le choix d'un vecteur normal donne une orientation pour \(G\) qui induit une orientation de l'hyperplan.

    \section{Produit mixte -- produit vectoriel}

    Soit \((E,\varphi)\) un espace vectoriel euclidien orienté de dimension \(n \in \N^*\).

    \subsection{Produit mixte}

    \subsubsection{Définition}

    \begin{lemme}
      Soient \(\B\) et \(\B'\) deux bases orthonormales de \(E\), alors \(\Det_\B(\B') \in \{-1; 1\}\). De plus
      \begin{itemize}
        \item si \(\B\) et \(\B'\) ont la même orientation alors \(\Det_\B(\B')=1\);
        \item sinon \(\Det_\B(\B')=-1\).
      \end{itemize}
    \end{lemme}
    \begin{proof}
      Notons \(\B=(b_1, \ldots, b_n)\), \(\B'=(b'_1, \ldots, b'_n)\), \(A=\P_{\B \B'}=\Mat_\B(\B')\). Alors pour tout \(j \in \intervalleentier{1}{n}\), on a
      \begin{equation}
        b'_j = \sum_{i=1}^na_{ij}b_i.
      \end{equation}
      Comme ce sont des bases orthonormées, on a pour tout \((j,k) \in \intervalleentier{1}{n}^2\)
      \begin{align}
        \prodscal{b_j}{b_k} &= \delta_{jk} \\
        \prodscal{b'_j}{b'_k} &= \delta_{jk} \label{eq:defBONdeltaij}.
      \end{align}
      Alors
      \begin{align}
        \delta_{jk} &= \prodscal{b'_j}{b'_k} && \text{BON}\\
                    &=\prodscal{\sum_{i=1}^na_{ij}b_i}{\sum_{l=1}^na_{lk}b_l} && \text{eq.} \eqref{eq:defBONdeltaij}\\
                    &=\sum_{i=1}^n\sum_{l=1}^n a_{ij}a_{kl} \prodscal{b_i}{b_l} && \text{bilinéarité}\\
                    &=\sum_{i=1}^na_{ij}a_{il} && \text{BON}\\
                    &=\sum_{i=1}^n (A^\top)_{ji}A_{ik} \\
                    &=(A^\top A)_{jk}.
      \end{align}
      Alors \(A^\top A=I_n\). Lorsqu'on passe au déterminant \(\Det(A) \in \{-1; 1\}\). De plus \(\B\) et \(\B'\) ont la même orientation si et seulement si \(\Det(A)>0\) c'est-à-dire \(\Det(A)=1\).
    \end{proof}
    \begin{lemme}
      Soit \(\X=(x_1, \ldots, x_n)\) une famille de \(n\) vecteurs de \(E\). Alors le réel \(\Det_\B(\X)\) est indépendant du choix de la base \(\B\) dans l'ensemble des bases orthonormées directes de \(E\).
    \end{lemme}
    \begin{proof}
      Soient \(\B\) et \(\B'\) deux bases orthonormées directes de \(E\). Alors
      \begin{equation}
        \Det_{\B'}(\X) = \Det_{\B'}(\B) \Det_{\B}(\X).
      \end{equation}
      Or \(\Det_{\B'}(\B)=1\) puisqu'elles sont de même orientations. Alors \(\Det_{\B'}(\X) = \Det_{\B}(\X)\).
    \end{proof}
    \begin{defdef}
      Soit \(X=(x_1, \ldots, x_n)\) une famille de \(n\) vecteurs de \(E\). On appelle produit mixte de \(\X\), et on note \(\Det(x_1, \ldots, x_n)\) le réel \(\Det_\B(\X)\), \(\B\) étant une base orthonormée directe quelconque de \(E\).
    \end{defdef}
    \begin{prop}
      L'application produit mixte
      \begin{equation}
        \fonction{\Det}{E^n}{\R}{x}{\Det(x)}
      \end{equation}
      est une forme \(n\)-linéaire alternée et antisymétrique.
    \end{prop}
    \begin{proof}
      Ce sont des propriétés de l'application \(\Det_\B\), \(\B\) étant une base orthonornée directe de \(E\).
    \end{proof}
    \begin{prop}
      Soit \(\X\) une famille de \(n\) vecteurs de \(E\), alors
      \begin{enumerate}
        \item \(\Det(\X)=0\) si et seulement si \(\X\) est liée;
        \item \(\Det(\X)>0\) si et seulement si \(\X\) est une base directe;
        \item \(\Det(\X)<0\) si et seulement si \(\X\) est une base indirecte.
      \end{enumerate}
    \end{prop}
    \begin{proof}
      Soit \(\B\) une base orthonormée directe de \(E\). Alors \(\Det(\X)=\Det_\B(\X)\). Donc \(\X\) est liée si et seulement si \(\Det_\B(\X)=0\). Si \(\X\) est libre alors \(\Det_\B(\X) \neq 0\). \(\Det_\B(\X)>0\) signifie que \(\X\) et \(\B\) ont la même orientation et comme \(\B\) est directe alors \(\X\) est directe. Idem pour indirecte.
    \end{proof}
    \begin{prop}
      Si on change l'orientation de \(E\), le produit mixte est changé en son opposé.
    \end{prop}
    \begin{proof}
      Soient \(\B_1\) et \(\B_2\) deux bases orthonormées d'orientation contraires. On note \(\Det_1\) le déterminant dans \(\B_1\) et \(\Det_2\) le déterminant dans \(\B_2\). Pour toute famille \(\X\) de \(n\) vecteurs de \(E\), on a
      \begin{align}
        \Det_1(\X) &= \Det_{\B_1}(\X) \\
        \Det_2(\X) &= \Det_{\B_2}(\X).
      \end{align}
      Donc
      \begin{equation}
        \Det_1(\X) = \Det_{\B_1}(\X) = \Det_{\B_1}(\B_2)\Det_{\B_2}(\X)=-\Det_2(\X).
      \end{equation}
    \end{proof}

    \subsubsection{Interprétation géométrique}

    \paragraph{Cas où \(n=2\)}

    Soit \((x,y)\) une famille libre de \(E\) (c.-à-d.\ une base car \(\Dim E=n=2\)). \(\Dr=\VectEngendre(x)\) et \(\Dr^\perp\) sont deux droites vectorielles car \(x\neq 0\) et \(\Dim(E)=2\). Alors \(E = \Dr \oplus \Dr^\perp\).

    Il existe un unique couple \((y_\Dr, y_{\Dr^\perp}) \in \Dr \times \Dr^\perp\) tel que \(y=y_\Dr +y_{\Dr^\perp}\). Le vecteur \(x\) est non nul, alors on peut poser \(i=\frac{x}{\norme{x}}\). On compléte \(i\) en une base orthonormale directe \((i,j)\) de \(E\).

    \begin{align}
      \Det(x,y) &= \Det(x,y_\Dr +y_{\Dr^\perp}) \\
                &=\Det(x,y_\Dr) + \Det(x,y_{\Dr^\perp}) && \text{linéarité à droite} \\
                &=\Det(x,y_{\Dr^\perp}). && (y_\Dr,x) \text{~est liée}
    \end{align}
    On a \(x=\norme{x}i\) et il existe \(\epsilon \in \{-1;1\}\) tel que \(y_{\Dr^\perp}=\epsilon\norme{y_{\Dr^\perp}}j\). Alors
    \begin{align}
      \Det(x,y) &=\Det(\norme{x}i, \epsilon\norme{y_{\Dr^\perp}}j) \\
                &=\norme{x} \epsilon\norme{y_{\Dr^\perp}} \Det(i,j) && \text{bilinéarité} \\
                &=\norme{x} \epsilon\norme{y_{\Dr^\perp}}.
    \end{align}
    Donc \(\abs{\Det(x,y)}=\norme{x}\norme{y_{\Dr^\perp}}\) et c'est l'aire du parallélogramme formé par \(x\) et \(y\). D'où la proposition suivante.

    \begin{prop}
      Pour tout couple \((x,y) \in E^2\), si \((x,y)\) est une famille libre alors \(\abs{\Det(x,y)}\) est égal à l'aire du parallèlogramme bâti sur \((x,y)\).
    \end{prop}

    \paragraph{Cas où \(n=3\)}

    \(E\) est un espace vectorel de dimension 3. Soit \((x,y,z)\) une famille libre de \(E\) (donc une base). \(\P=\VectEngendre(x,y)\) est un plan vectoriel et \(\P^\perp\) est une droite vectoriel (car \(\Dim E=3\)). Alors \(E =\P \oplus\P^\perp\).

    Alors il existe un unique couple \((z_\P,z_{\P^\perp}) \in \P \times \P^\perp\) tel que \(z=z_\P +z_{\P^\perp}\). Ainsi
    \begin{equation}
      \Det(x,y,z) = \Det(x,y,z_\P) +\Det(x,y,z_{\P^\perp})) = \Det(x,y,z_{\P^\perp})),
    \end{equation}
    car \(z_\P \in \P\).

    La famille \((x,y,z)\) est libre donc \(z \notin \P\) et donc \(z_{\P^\perp} \neq 0\). On pose \(k=\frac{z_{\P^\perp}}{\norme{z_{\P^\perp}}}\). On compléte \(k\) en une base orthonormale \((i,j,k)\) de \(E\). \((i,j)\) est une base orthonormale de \(\P\). Alors on a
    \begin{align}
      \Det(x,y,z) &= \Det(x,y,k\norme{z_{\P^\perp}}) \\
                  &=\norme{z_{\P^\perp}}\Det(x,y,k) && \text{trilinéarité}\\
                  &=\norme{z_{\P^\perp}}\Det_{(i,j,k)}(x,y,k)\\
                  &=\norme{z_{\P^\perp}}\Det_{(i,j)}(x,y) && \norme{k}=1\\
                  &=\norme{z_{\P^\perp}}\Det(x,y).
    \end{align}
    Alors
    \begin{equation}
      \abs{\Det(x,y,z)} = \norme{z_{\P^\perp}}\abs{\Det(x,y)}.
    \end{equation}
    D'où la proposition suivante
    \begin{prop}
      Pour tout couple \((x,y,z) \in E^3\), si \((x,y,z)\) est une famille libre alors \(\abs{\Det(x,y,z)}\) est égal au volume du parallélépipéde bâti sur \((x,y,z)\).
    \end{prop}

    \subsection{Produit vectoriel, en dimension 3}

    Soit \((E,\varphi)\) un espace euclidien orienté de dimension 3.

    \subsubsection{Définition}

    \begin{defdef}
      Soit \((x,y) \in E^2\). On appelle produit vectoriel de \(x\) et \(y\), dans cet ordre, et on note \(x \wedge y\) le vecteur normal de la forme linéaire
      \begin{equation}
        \fonction{f}{E}{\R}{z}{\Det(x,y,z)}.
      \end{equation}
      Et c'est légitime, car~:
      \begin{itemize}
        \item le produit vectoriel est défini, car \(E\) est orienté;
        \item \(f\) est une forme linéaire, car \(\Det\) est trilinéaire;
        \item \(f\) admet un unique vecteur normal.
      \end{itemize}
      Ainsi \(x \wedge y\) est l'unique vecteur \(p \in E\) tel que pour tout \(z \in E\) on ait \(f(z)=\Det(x,y,z)=\prodscal{p}{z}\).
    \end{defdef}
    \begin{prop}
      Pour triplet de vecteurs \((x,y,z) \in E^3\) on a
      \begin{gather}
        \Det(x,y,z)=\prodscal{x\wedge y}{z}; \\
        \prodscal{y\wedge x}{z} = -\prodscal{x\wedge y}{z}; \\
        \prodscal{y\wedge z}{x} = \prodscal{z\wedge x}{y} =\prodscal{x\wedge y}{z}.
      \end{gather}
    \end{prop}
    \begin{proof}
      La première égalit'' est due à la définition du produit vectoriel. On montre la deuxième~:
      \begin{align}
        \prodscal{y\wedge x}{z} &=\Det(y,x,z)\\
                                &=-\Det(x,y,z)&& \text{antisymétrie}\\
                                &=-\prodscal{x\wedge y}{z}.
      \end{align}
      La deuxième~:
      \begin{align}
        \prodscal{y\wedge z}{x} &=\Det(y,z,x) \\
                                &=\Det(x,y,z)\\
                                &=\prodscal{x\wedge y}{z}.
      \end{align}
      Idem pour le reste.
    \end{proof}
    \begin{prop}[Effet d'un changement d'orientation de \(E\)]
      Soit \((x,y) \in E^2\). Si on change l'orientation de \(E\), \(x \wedge y\) est changé en son opposé.
    \end{prop}
    \begin{proof}
      Si on change l'orientation de \(E\), l'application produit mixte \(\Det\) est changée en son opposée. Donc la forme linéaire \(f\) est changée en son opposée. Par conséquent le vecteur normal de \(f\) est aussi changé en son opposé.
    \end{proof}

    \subsubsection{Propriétés du vecteur \(x \wedge y\)}

    \begin{theo}
      Soit \((x,y) \in E^2\). Alors
      \begin{enumerate}
        \item \(x \wedge y \perp x\) et \(x \wedge y \perp y\);
        \item \(x \wedge y = 0\) si et seulement si \((x,y)\) est liée;
        \item Si \((x,y)\) est libre, alors \((x,y,x \wedge y)\) est une base directe de \(E\).
      \end{enumerate}
    \end{theo}
    \begin{proof}
      \begin{enumerate}
        \item En effet, on a \(\prodscal{x \wedge y}{x}=\Det(x,y,x)=0\) donc \(x \wedge y \perp x\). Idem pour \(y\).
        \item \(\impliedby\). Si \((x,y)\) est liée, alors \((x,y,x\wedge y)\) est liée aussi et donc
          \begin{align}
            \Det(x,y,x\wedge y) &= 0\\
            \prodscal{x\wedge y}{x\wedge y} &= 0 \\
            \norme{x\wedge y}^2=0,
          \end{align}
          et donc \(x \wedge y=0\).

          \(\implies\). Par contraposée. Supposons que \('x,y)\) est libre. Alors il existe \(z \in E\) tel que \((x,y,z)\) est une base de \(E\). Ainsi \(\Det(x,y,z)=\prodscal{x\wedge y}{z}\neq 0\). Par conséquent \(x\wedge y \neq 0\).
        \item Si \((x,y)\) est libre, \(\Det(x,y,x \wedge y)=\norme{x \wedge y}^2 >0\) (car \(x \wedge y \neq 0\)). Donc \((x,y,x \wedge y)\) est une base directe de \(E\).
      \end{enumerate}
    \end{proof}

    \subsubsection{Propriétés du produit vectoriel}

    \begin{theo}
      L'application \(\fonction{\wedge}{E^2}{E}{(x,y)}{x \wedge y}\) est bilinéaire et alterné (donc antisymétrique).
    \end{theo}
    \begin{proof}
      Soient \((x,x',y) \in E^3\) et \(\lambda \in \R\). On note \(p_1=(\lambda x+x')\wedge y\) et \(p_2=\lambda x\wedge y+x'\wedge y\). Pour tout vecteur \(z \in E\), on a
      \begin{align}
        \prodscal{p_1}{z} &=\prodscal{(\lambda x+x')\wedge y}{z} \\
                          &=\Det(\lambda x +x',y,z) && \text{définition}\\
                          &=\lambda \Det(x,y,z) +\Det(x',y,z) && \text{trilinéarité}\\
                          &=\lambda \prodscal{x \wedge y}{z} + \prodscal{x' \wedge y}{z} \\
                          &=\prodscal{\lambda x\wedge y +x'\wedge y}{z}\\
                          &=\prodscal{p_2}{z}.
      \end{align}
      Alors \(\prodscal{p_1-p_2}{z}=0\) et comme c'est vrai pour tout \(z\) alors \(p_1=p_2\). C'est la linéarité à gauche.

      Soient \((x,y) \in E^2\). Pour tout \(z \in E\), on a
      \begin{align}
        \prodscal{y \wedge x}{z} &= \Det(y,x,z) \\
                                 &=-\Det(x,y,z) && \text{antisymétrie} \\
                                 &=-\prodscal{x \wedge y}{z}\\
                                 &=\prodscal{-x \wedge y}{z}.
      \end{align}
      Alors \(\prodscal{y \wedge x+x \wedge y}{z}=0\) et comme c'est vrai pour tout \(z \in E\) on a bien \(x \wedge y = -y\wedge x\). C'est l'antisymétrie.

      La linéarité à gauche et l'antisymétrie nous donne la linéarité à droite.
    \end{proof}

    \subsubsection{Coordonnées de \(x \wedge y\) dans une base orthonormale directe}

    Soit \(\B=(b_1,b_2,b_3)\) une base orthonormée directe de \(E\). Soient \(x\) et \(y\) dans \(E\) de coordonnées respectives \((x_1,x_2,x_3)\) et \((y_1,y_2,y_3)\) dans la base \(\B\). Alors
    \begin{equation}
      x \wedge y = \sum_{i=1}^3 \prodscal{x \wedge y}{b_i}b_i.
    \end{equation}
    Car \(\B\) est une base orthonormée. Alors on a
    \begin{align}
      \prodscal{x \wedge y}{b_1} &=\Det(x,y,b_1) \\
                                 &=\Det_{b_1,b_2,b_3}(x,y,b_1)\\
                                 &=\begin{vmatrix} x_2 & y_2 \\ x_3 & y_3 \end{vmatrix}.
    \end{align}
    De la même manière on trouve
    \begin{equation}
      \prodscal{x \wedge y}{b_2} = - \begin{vmatrix} x_1 & y_1 \\ x_3 & y_3 \end{vmatrix} \qquad \prodscal{x \wedge y}{b_3} =  \begin{vmatrix} x_1 & y_1 \\ x_2 & y_2 \end{vmatrix}.
    \end{equation}
    D'où la proposition suivante
    \begin{prop}
      Avec les notations précédentes, les coordonnées de \(x \wedge y\) dans la base orthonormée directe \(\B\) sont \(\left(\begin{vmatrix} x_2 & y_2 \\ x_3 & y_3 \end{vmatrix}, - \begin{vmatrix} x_1 & y_1 \\ x_3 & y_3 \end{vmatrix},  \begin{vmatrix} x_1 & y_1 \\ x_2 & y_2 \end{vmatrix}\right)\).
    \end{prop}

    \subsubsection{Propriétés relatives aux bases orthonormées directes}

    \begin{prop}
      Soit \((i,j,k)\) une base orthonormée directe. Alors \(i \wedge j=k\), \(j \wedge k = i\), et \(k \wedge i =j\).
    \end{prop}
    \begin{proof}
      La base \((i,j,k)\) est orthonormée donc
      \begin{align}
        i \wedge j &= \prodscal{i \wedge j}{i}i + \prodscal{i \wedge j}{j}j + \prodscal{i \wedge j}{k}k \\
                   &=\prodscal{i \wedge j}{k}k\\
                   &=\Det(i,j,k)k\\
                   &=\Det_{(i,j,k)}(i,j,k)k\\
                   &=k.
      \end{align}
      Idem pour les autres.
    \end{proof}
    \begin{prop}
      Soient \(x\) et \(y\) deux vecteurs de \(E\) unitaires et orthogonaux. Alors \((x,y,x \wedge y)\) est ue base orthonormée directe.
    \end{prop}
    \begin{proof}
      La famille \((x,y)\) est orthogonale et \(x\) et \(y\) sont non nuls. Donc Elle est libre. On sait déjà que \((x,y,x \wedge y)\) est une base directe de \(E\). Par hypothèse \(x \perp y\) et d'après les propriétés on a \(x \perp x \wedge y\) et \(y \perp x \wedge y\). Alors \((x,y,x \wedge y)\) est une base orthogonale. Ainsi \(\left(x,y,\frac{x \wedge y}{\norme{x \wedge y}}\right)\) est une base orthonormée directe.
    \end{proof}
    \begin{prop}
      Soit \((x,y) \in E^2\). Si \(x \perp y\) alors \(\norme{x \wedge y}=\norme{x}\cdot\norme{y}\).
    \end{prop}
    \begin{proof}
      Deux cas se présentent. Si \(x\) est nul ou si \(y\) est nul alors \(x \wedge y\) est nul. L'égalité est vraie.

      Sinon, c'est-à-dire si \(x\) est non nul et si \(y\) est non nul, alors \(\frac{x}{\norme{x}}\) et \(\frac{y}{\norme{y}}\) sont unitaires et orthogonaux. En appliquant la proposition précédente \(\left(\frac{x}{\norme{x}}, \frac{y}{\norme{y}}, \frac{x}{\norme{x}}\wedge \frac{y}{\norme{y}}\right)\) est une base orthonormée directe de \(E\). En particulier \(\frac{x}{\norme{x}}\wedge \frac{y}{\norme{y}}\) est unitaire et donc
      \begin{equation}
        1 = \frac{\norme{x \wedge y}}{\norme{x}\norme{y}}.
      \end{equation}
      Alors \(\norme{x \wedge y}=\norme{x}\cdot\norme{y}\).
    \end{proof}

    \subsubsection{Double produit vectoriel}

    \begin{theo}
      Pour tout triplet \((x,y,z) \in E^3\) on a
      \begin{equation}
        (x \wedge y)\wedge z = \prodscal{x}{z}y-\prodscal{y}{z}x.
      \end{equation}
    \end{theo}
    \begin{proof}
      Voir le chapitre~
      \ref{chap:geomEspace}.
    \end{proof}
    \emph{Remarque~:} Pour tout triplet \((x,y,z) \in E^3\) on a
    \begin{equation}
      x \wedge (y \wedge z) = \prodscal{x}{z}y-\prodscal{x}{y}z.
    \end{equation}
